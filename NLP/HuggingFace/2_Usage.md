## Introduction

At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.


## Behind the pipeline
![](Pasted%20image%2020210629143959.png)


### tokenizer
ä½œç”¨ï¼š
* åˆ†è¯ï¼Œå¾—åˆ°token
* æŠŠtokenæ˜ å°„åˆ°integer
* Adding additional inputs that may be useful to the model

å¯¹tokençš„å¤„ç†è¦å’Œpretrain modelä¸€è‡´ï¼Œå› æ­¤è¦æŸ¥pretrainçš„ä¿¡æ¯ã€‚
 [Model Hub](https://huggingface.co/models).


To do this, we use the `AutoTokenizer` class and its `from_pretrained` method.

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ä»£ç ç¬¬ä¸€æ¬¡è¿è¡Œçš„æ—¶å€™ï¼Œåˆ©ç”¨checkpointçš„åå­—ä¸‹è½½model

ç»è¿‡tokenizerï¼ŒæŠŠæ–‡æœ¬è½¬æˆid

transformer modelæ¥æ”¶çš„æ˜¯tensorï¼Œè¦æŒ‡å®šbackendæ˜¯kerasè¿˜æ˜¯pytorchï¼Œè¦åœ¨tokenizerä¸­è®¾å®š`return_tensors`.
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```


Hereâ€™s what the results look like as PyTorch tensors:
```python
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
The output itself is a dictionary containing two keys, `input_ids` and `attention_mask`.


### Going through the model


åŒæ ·ï¼Œé€šè¿‡`from_pretrained`æ¥ä¸‹è½½model
```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```

Transformerçš„è¾“å‡ºé€šå¸¸æœ‰3ç»´ï¼šBatch size, Sequence length, Hidden size.



```python

outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```
Note that the outputs of ğŸ¤— Transformers models behave like `namedtuple`s or dictionaries. You can access the elements by attributes (like we did) or by key (`outputs["last_hidden_state"]`), or even by index if you know exactly where the thing you are looking for is (`outputs[0]`).


### Model heads: Making sense out of numbers

model headæ˜¯å¯¹å‰é¢çš„modelè¾“å‡ºçš„hidden stateä½œå¤„ç†çš„éƒ¨åˆ†

![](Pasted%20image%2020210629151822.png)



There are many different architectures available in ğŸ¤— Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:

-   `*Model` (retrieve the hidden states)
-   `*ForCausalLM`
-   `*ForMaskedLM`
-   `*ForMultipleChoice`
-   `*ForQuestionAnswering`
-   `*ForSequenceClassification`
-   `*ForTokenClassification`
-   and others ğŸ¤—


å‡è®¾æˆ‘ä»¬åšæ–‡æœ¬åˆ†ç±»ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥ç”¨`AutoModelForSequenceClassification`è€Œä¸æ˜¯`AutoModel` classã€‚


```python

from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)


```


è¿™æ—¶åŸæœ¬çš„N*batch_size*768çš„hidden stateå°±ç»è¿‡head layerå˜æˆäº†N*2çš„tensor (å› ä¸ºæ˜¯classificationçš„one hotæ‰€ä»¥æ˜¯2)



### Postprocessing the output


(**all ğŸ¤— Transformers models output the logits**, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy)

å› æ­¤ä½¿ç”¨æ—¶è¦æŠŠlogitsç»è¿‡softmax

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

```

To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config (more on this in the next section):
```python
model.config.id2label

{0: 'NEGATIVE', 1: 'POSITIVE'}
```


## Models
pretrained modelåŠ è½½çš„è¿‡ç¨‹ï¼š
![](Pasted%20image%2020210629153157.png)



å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹æ³•è°ƒå‚ï¼Œç„¶åtrain from scratchï¼š
![](Pasted%20image%2020210629153638.png)

saveå’Œloadçš„æ–¹æ³•ï¼š
![](Pasted%20image%2020210629153709.png)



ä¹‹å‰ç”¨çš„AutoModelæ˜¯åˆ©ç”¨ä¼ å…¥çš„checkpointåˆ¤æ–­ç”¨å“ªä¸ªmodelï¼Œç„¶åè‡ªåŠ¨åŠ è½½config
å…¶å®ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šï¼š
```python
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)

```
æ³¨æ„æ­¤æ—¶modelçš„weightæ˜¯éšæœºç”Ÿæˆçš„ï¼Œæ²¡æœ‰åŠ è½½pretrainedå‚æ•°

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")

```

æœ€å¥½æ˜¯ç”¨AutoModelç›´æ¥ä¼ checkpoint name

ä¸‹è½½ä¸‹æ¥çš„modelçš„è·¯å¾„ï¼š
The weights have been downloaded and cached (so future calls to the `from_pretrained` method wonâ€™t re-download them) in the cache folder, which defaults to _~/.cache/huggingface/transformers_. You can customize your cache folder by setting the `HF_HOME` environment variable.

ä¿å­˜modelï¼š
model.save_pretrained("directory_on_my_computer")

### Inference

```python
sequences = [
  "Hello!",
  "Cool.",
  "Nice!"
]

encoded_sequences = [
  [ 101, 7592,  999,  102],
  [ 101, 4658, 1012,  102],
  [ 101, 3835,  999,  102]
]

import torch

model_inputs = torch.tensor(encoded_sequences)

output = model(model_inputs)





```




## Tokenizers

### Word-based
we need a custom token to represent words that are not in our vocabulary. This is known as the â€œunknownâ€ token, often represented as â€[UNK]â€ or â€â€. Itâ€™s generally a bad sign if you see that the tokenizer is producing a lot of these tokens

One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer.


### Character-based

* åœ¨æœ‰äº›è¯­è¨€ä¸­ï¼Œcharacteræ„ä¹‰å¾ˆå°
* æ–‡æœ¬å˜é•¿

### Subword tokenization

**frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.**

### And more!

Unsurprisingly, there are many more techniques out there. To name a few:

-   Byte-level BPE, as used in GPT-2
-   WordPiece, as used in BERT
-   SentencePiece or Unigram, as used in several multilingual models

### Loading and saving
`from_pretrained` and `save_pretrained`

Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

tokenizer("Using a Transformer network is simple")

{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}

```

### Encoding
åˆ†è¯:
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)




```
è½¬id:
```python
ids = tokenizer.convert_tokens_to_ids(tokens)

print(ids)

```



### Decoding

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)

```

Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence.



## Handling multiple sequences

modelé»˜è®¤æ¥æ”¶batchä½œä¸ºè¾“å…¥ã€‚

The padding token ID can be found in `tokenizer.pad_token_id`.



æ³¨æ„:å•ç‹¬ç”¨tokenizer.pad_token_idï¼Œä¼šå¯¹ç»“æœæœ‰å½±å“ï¼Œå› æ­¤è¦ç»“åˆmask
```python
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [[200, 200, 200], [200, 200, tokenizer.pad_token_id]]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)


tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)



```


```python

batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
  [1, 1, 1],
  [1, 1, 0]
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)

tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)


```

With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens

Models have different supported sequence lengths, and some specialize in handling very long sequences. [Longformer](https://huggingface.co/transformers/model_doc/longformer.html) is one example, and another is [LED](https://huggingface.co/transformers/model_doc/led.html).

Otherwise, we recommend you truncate your sequences by specifying the `max_sequence_length` parameter:

sequence = sequence[:max_sequence_length]



## Putting it all together


å¯ä»¥æŠŠsentenceçš„batchç›´æ¥ä¼ å…¥tokenizerï¼Œè¿™ä¸ªmethodå¾ˆå¼ºå¤§ã€‚
å¯ä»¥å¤„ç†å•ä¸ªå¥å­ï¼Œä¹Ÿå¯ä»¥è‡ªåŠ¨åˆ¤æ–­ï¼Œå¤„ç†å¤šä¸ªå¥å­ã€‚
```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

model_inputs = tokenizer(sequences)

```


å¯ä»¥paddingï¼š
```python
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å¯ä»¥truncateï¼š
```python
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)

```
 è§„å®šè¿”å›çš„tensorç±»å‹ï¼š
 ```python
 sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```


è‡ªåŠ¨çš„tokenizerè¦æ¯”æ‰‹åŠ¨çš„å¤šåŠ BEGINå’ŒEND

æ€»ç»“ï¼š
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
  "I've been waiting for a HuggingFace course my whole life.",
  "So have I!"
]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)




```







